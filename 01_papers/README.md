# Papers
This section contains a colleciton of research papers. Some of them are provided with summaries.

|Title|Author|Year|Link|
|-----|-----|----|----| 
|Structcoder: Structure-aware transformer for code generation| Tipirneni et al.| 2024 |[ACM](https://dl.acm.org/doi/pdf/10.1145/3636430)|
|Learning and Evaluating Contextual Embedding of Source Code|Kanade et al.| 2019 |[arXiv](https://arxiv.org/pdf/2001.00059.pdf)|
|Fuzz4All: Universal Fuzzing with Large Language Models | Xia et al. | 2024 |[arXiv](https://arxiv.org/pdf/2308.04748.pdf) |
|Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models | Deng et al. | 2023 | [arXiv](https://arxiv.org/abs/2212.14834)|
|Large Language Models for Code: Security Hardening and Adversarial Testing | He et al. | 2024 |[arXiv](https://arxiv.org/abs/2302.05319)|
|SymLM: Predicting Function Names in Stripped Binaries via Context-Sensitive Execution-Aware Code Embeddings| Jin et al. | 2022 | [ACM](https://dl.acm.org/doi/pdf/10.1145/3548606.3560612) | 
|TRACED: Execution-aware Pre-training for Source Code|Ding et al.|2024|[arXiv](https://arxiv.org/pdf/2306.07487.pdf)|
|CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation|Lu et al.|2021|[arXiv](https://arxiv.org/pdf/2102.04664.pdf)|
|RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair|Silva et al|2023|[arXiv](https://arxiv.org/pdf/2312.15698)|
|Grace: Language Models Meet Code Edits|Gupta et al.|2023|[arXiv](https://arxiv.org/pdf/2305.14129.pdf)|
|SkipAnalyzer: A Tool for Static Code Analysis with Large Language Models|Mohajer et al|2023|[arXiv](https://arxiv.org/pdf/2310.18532)|
|Studying LLM Performance on Closed- and Open-source Data|Ahmed et al.|2024|[arXiv](https://arxiv.org/pdf/2402.15100.pdf)|
|Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search|Li et al.|2024|[arXiv](https://arxiv.org/pdf/2401.04514.pdf)|
|Automatically Testing Functional Properties of Code Translation Models|Eniser et al.|2023|[arXiv](https://arxiv.org/pdf/2309.12813.pdf)|
|Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation|Li et al|2023|[arXiv](https://arxiv.org/pdf/2305.10679.pdf9)|
|Rethinking Negative Pairs in Code Search|Li et al.|2023|[arXiv](https://arxiv.org/pdf/2310.08069.pdf)|
|Supersonic: Learning to Generate Source Code Optimizations in C/C++|Chen et al|2023|[arXiv](https://arxiv.org/pdf/2309.14846)|
|Guiding Language Models of Code With Global Context using Monitors|Agarwal et al|2023|[arXiv](https://arxiv.org/pdf/2306.10763.pdf)|
|OctoPack: Instruction Tuning Code Large Language Models|Muennighoff et al|2023|[arXiv](https://arxiv.org/pdf/2308.07124.pdf)|
|The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models|Li et al|2023|[arXiv](https://arxiv.org/pdf/2308.00245)|
|LongCoder: A Long-Range Pre-trained Language Model for Code Completion|Guo et al.|2023|[arXiv](https://arxiv.org/pdf/2306.14893.pdf)|
|LLM4Decompile: Decompiling Binary Code with Large Language Models|Tan et al|2024|[arXiv](https://arxiv.org/pdf/2403.05286.pdf)|
|A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks|Casey et al|2024|[arXiv](https://arxiv.org/pdf/2403.10646.pdf)|
|AutoDev: Automated AI-Driven Development|Tufano et al|2024|[arXiv](https://arxiv.org/pdf/2403.08299.pdf)|
|Representation and Generation of Machine Learning Test Functions|Hassine et al|2024|[arXiv](Representation and Generation of Machine Learning Test Functions)|
|Leveraging pre-trained language models for code generation|Soliman et al|2024|[Springer](https://link.springer.com/article/10.1007/s40747-024-01373-8)|
|Programming Assistant for Exception Handling with CodeBERT|Cai et al.|2024|[ICSE](https://www.computer.org/csdl/proceedings-article/icse/2024/021700a878/1V5BkrGwfJu)|
|T5APR: Empowering Automated Program Repair Across Languages Through Checkpoint Ensemble|Gharibi et al.|2024|[arXiv](https://arxiv.org/pdf/2309.15742.pdf)|
|Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants|Corso et al.|2024|[arXiv](https://arxiv.org/pdf/2402.08431.pdf)|
|IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators|Paul et al.|2024|[IRCoder](https://arxiv.org/pdf/2403.03894.pdf)|
|CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences|Weyssow et al|2024|[arXiv](https://arxiv.org/pdf/2403.09032.pdf)|
|Conversational Assistants for Software Development: Integration, Traceability and Coordination|Contreras et al.|2024|[arXiv](https://miso.es/pubs/enase.pdf)|
|Improving Code Smell Detection Using Deep Stacked Autoencoder|Rehef et al.|2024|[preprints.org](https://www.preprints.org/manuscript/202403.1848/v1)|
|DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models|Berabi et al|2024|[arXiv](https://arxiv.org/pdf/2402.13291.pdf)|
|RepairAgent: An Autonomous, LLM-Based Agent for Program Repair|Bouzenia et al|2024|[arXiv](https://arxiv.org/pdf/2403.17134.pdf)|
|Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions|Cassano et al|2023|[arXiv](https://arxiv.org/pdf/2312.12450.pdf)|
|DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence|Guo et al|2024|[arXiv](https://arxiv.org/pdf/2401.14196.pdf)|
|AutoCodeRover: Autonomous Program Improvement|Zhang et al|2024|[arXiv](https://arxiv.org/pdf/2404.05427.pdf), [GitHub](https://github.com/nus-apr/auto-code-rover)|
|Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code|Majdinasab et al.|2024|[GitHub](https://github.com/CommissarSilver/TraWiC), [arXiv](https://arxiv.org/pdf/2402.09299.pdf)|
|LongEmbed: Extending Embedding Models for Long Context Retrieval|Zhu et al|2024|[arXiv](https://arxiv.org/abs/2404.12096), [GitHub](https://github.com/dwzhu-pku/LongEmbed)|
|Out of the BLEU: How Should We Assess Quality of the Code Generation Models?|JetBrains|2023|[arXiv](https://arxiv.org/pdf/2208.03133.pdf)|
|NeuralSVG: An Implicit Representation for Text-to-Vector Generation|Polaczek et al|2025|[arXiv](https://arxiv.org/abs/2501.03992)|
|ModernBERT -Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference|Warner et al|2024|[arXiv](https://arxiv.org/abs/2412.13663)|
|CanIt Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions|Cassano et al|2024|[arXiv](https://arxiv.org/pdf/2312.12450), [GitHub](https://github.com/nuprl/CanItEdit)|
|Unsupervised Evaluation of Code LLMs with Round-Trip Correctness|Allamanis et al|2024|[arXiv](https://arxiv.org/abs/2402.08699)|
|Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, Pengcheng Yin|Ni et al|2024|[arXiv](https://arxiv.org/abs/2404.14662)|
|HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation|Yu et al|2024|[arXiv](https://arxiv.org/abs/2412.21199)|
|Magicoder: Empowering Code Generation with OSS-Instruct|Wei et al|2024|[Proceedings](https://proceedings.mlr.press/v235/wei24h.html), [GitHub](https://github.com/ise-uiuc/magicoder)|
|SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents|Mündler et al.|2024|[ArXiv](https://arxiv.org/abs/2406.12952), [GitHub](https://github.com/logic-star-ai/swt-bench)|
|CodeSage: Code Representation at Scale|Zhang et al.|2024|[GitHub](https://github.com/amazon-science/CodeSage), [Website](https://code-representation-learning.github.io/), [arXiv](https://arxiv.org/abs/2402.01935)|
|Learning from Large Codebases|Veselin Raychev|2024|[ETHZ Zürich](https://files.sri.inf.ethz.ch/website/people/veselin/raychev_thesis.pdf)|
|AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation|Murali et al|2024|[arXiv](https://arxiv.org/pdf/2305.12050)|
|Automated Unit Test Improvement using Large Language Models at Meta|Alshahwan et al|2025|[arXiv](https://arxiv.org/pdf/2402.09171)|
|Competitive Programming with Large Reasoning Models|OpenAI|2025|[arXiv](https://arxiv.org/pdf/2502.06807v1)|
|EquiBench:Benchmarking Code Reasoning Capabilities of Large Language Models via Equivalence Checking|Wei et al|2025|[arXiv](https://arxiv.org/pdf/2502.12466)|
|GNN-Coder: Boosting Semantic Code Retrieval with Combined GNN and Transformer|Yet et al|2025|[arXiv](https://arxiv.org/pdf/2502.15202)|
|Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs|Yang et al|2025|[arXiv](https://arxiv.org/pdf/2502.19411)|
|Large Language Models for Code Analysis: Do LLMs Really Do Their Job?|Fang et al|2024|[arXiv](https://arxiv.org/pdf/2310.12357)|
|Code Summarization: Do transformers really understand code?|Sontakke et al|2022|[arXiv](https://openreview.net/pdf?id=rI5ll2_-1Zc)|
|BaxBench: Can LLMs Generate Correct and Secure Backends?|Vero et al.|2025|[arXiv](https://arxiv.org/abs/2502.11844)|
|Testing the Effect of Code Documentation on Large Language Model Code Understanding|Macke et al|2024|[arXiv](https://arxiv.org/abs/2404.03114v1)|
|StarVector: Generating Scalable Vector Graphics Code From Images And Text|Rodriguez et al|2025|[arXiv](https://arxiv.org/abs/2312.11556), [Website](https://starvector.github.io/)
|Type-Constrained Code Generation with Language Models| Mündler et al|2025|[Openreview](https://openreview.net/pdf?id=LYVyioTwvF), [arXiv](https://arxiv.org/pdf/2504.09246)|
|InCoder: A Generative Model for Code Infilling and Synthesis|Fried et al.|2023|[arXiv](https://arxiv.org/pdf/2204.05999)|
|Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo||2025|[arXiv](https://arxiv.org/pdf/2504.13139)|
|RustAssistant: Using LLMs to Fix Compilation Errors in Rust Code|Deligiannis et al.|2025|[Microsoft](https://www.microsoft.com/en-us/research/wp-content/uploads/2024/08/paper.pdf)|
|Gorilla: Large Language Model Connected with Massive APIs|Patil et al|2025|[arXiv](https://arxiv.org/pdf/2305.15334)|
|Challenges and Paths Towards AI for Software Engineering|Gu et al|2025|[GitHub](https://minimario.github.io/papers/challenges_ai4se.pdf)|
|MathConstruct: Challenging LLM Reasoning with Constructive Proofs|Balunovic et al|2025|[arXiv](https://arxiv.org/pdf/2502.10197)|
|SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering|Yang et al|2024|[arXiv](https://arxiv.org/abs/2405.15793)|
|Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey|Dou et al.|2023|[arXiv](https://arxiv.org/pdf/2308.01191.pdf)|
|A systematic literature review on source code similarity measurement and clone detection: techniques, applications, and challenges|Zakeri-Nasrabadi et al.|2023|[arXiv](https://arxiv.org/ftp/arxiv/papers/2306/2306.16171.pdf)|
|Code Search : A Survey of Techniques for Finding Code|Di Garzia et al.|2022|[arXiv](https://arxiv.org/pdf/2204.02765.pdf)|
|Code Search (Dagstuhl Seminar 24172)|Chandra et al.|2024|[Dagstuhl](https://drops.dagstuhl.de/storage/04dagstuhl-reports/volume14/issue04/24172/DagRep.14.4.108/DagRep.14.4.108.pdf)|
|A survey on deep learning approaches for text-to-SQL|Katsogniannis et al.|2023|[Springer](https://link.springer.com/content/pdf/10.1007/s00778-022-00776-8.pdf)|
|Few-shot Text-to-SQL Translation using Structure and Content Prompt Learning|Gu et al.|2023|[Renmin University](http://iir.ruc.edu.cn/~fanj/papers/sigmod2023-scprompt.pdf)|
|NL2SQL is a solved problem... Not!|Floratou et al.|2024|[CIDR](https://www.cidrdb.org/cidr2024/papers/p74-floratou.pdf)|
|CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL|Pourreza et al.|2024|[arXiv](https://arxiv.org/abs/2410.01943)|
|The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models|Maamari et al.|2024|[arXiv](https://arxiv.org/abs/2408.07702)|
|CHESS: Contextual Harnessing for Efficient SQL Synthesis|Talaei et al.|2024|[arXiv](https://arxiv.org/abs/2405.16755)|
|E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL|Cafteroglu et al.|2024|[arXiv](https://arxiv.org/abs/2409.16751)|
|RSL-SQL: Robust Schema Linking in Text-to-SQL Generation|Cao et al.|2024|[arXiv](https://arxiv.org/abs/2411.00073)|
|Route: Robust Multitask Tuning and Collaboration for Text-to-SQL | Qin et al.| 2025|[OpenReview](https://openreview.net/pdf?id=BAglD6NGy0)|
|Automated Unit Test Improvement using Large Language Models at Meta|Alshahwan et al. (Meta)|2024|[arXiv](https://arxiv.org/pdf/2402.09171.pdf), [Engineer's Codex](https://read.engineerscodex.com/p/metas-new-llm-based-test-generator)|