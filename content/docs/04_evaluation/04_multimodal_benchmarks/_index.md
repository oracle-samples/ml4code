---
title: Multimodal Benchmarks
url: multimodal-benchmarks
weight: 4
---

## Multimodal

| Title | Description | Links |
|---|---|---|
|[AI2D](https://ai2-website.s3.amazonaws.com/publications/Diagrams_ECCV2016.pdf)|5,000 diagrams with exhaustive annotations of constituents and relationships and 15,000 questions and answers.|[Website](https://prior.allenai.org/projects/diagram-understanding)|
|[BLINK](https://arxiv.org/abs/2404.12390.pdf)|Reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting|[Website](https://zeyofu.github.io/blink/)|
|[ChartQA](https://arxiv.org/abs/2203.10244)|9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries.|[Website](https://github.com/vis-nlp/ChartQA)|
|[COCOcap](https://arxiv.org/pdf/1405.0312)|330K images with 5 captions per image|[Website](https://arxiv.org/pdf/1405.0312), [GitHub](https://github.com/tylin/coco-caption)|
|[CountBench QA](https://arxiv.org/pdf/2407.07726v1)|There are 540 images in total with 60 images per count in [2, 3, ... 10]|[Websites](https://teaching-clip-to-count.github.io/)|
|[MMMU](https://arxiv.org/abs/2311.16502)|11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks. The q uestions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.|[Website](https://mmmu-benchmark.github.io/)|
|[DocVQA](https://arxiv.org/pdf/2007.00398)|50,000 questions defined on 12,000+ document images.|[Website](https://www.docvqa.org/)|
|[InfoVQA](https://arxiv.org/pdf/2104.12756)|30035 questions over 5485 images.|[Website](https://www.docvqa.org/datasets/infographicvqa)|
|[OKVQA](https://arxiv.org/pdf/2206.01718)|OK-VQA is a new dataset for visual question answering that requires methods which can draw upon outside knowledge to answer questions. 14,055 open-ended questions. 5 ground truth answers per question|[Website](https://okvqa.allenai.org/)|
|[RealWorldQA](https://x.ai/news/grok-1.5v)|700 images, with a question and easily verifiable answer for each image.|[Dataset](https://data.x.ai/realworldqa.zip)|
|[ReMI](https://arxiv.org/pdf/2406.09175)|13 taks for math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning.|[HuggingFace](https://huggingface.co/datasets/mehrankazemi/ReMI)|
|[SpatialSense](https://arxiv.org/abs/1908.02660)|17,498 relations on 11,569 images|[GitHub](https://github.com/princeton-vl/SpatialSense)|
|[TallyQA](https://arxiv.org/abs/1810.12440)|287K Simple and complex counting questions on 165K images. |[GitHub](https://github.com/manoja328/TallyQA_datasets), [Website](https://www.manojacharya.com/tallyqas)|
|[VQA](http://visualqa.org/)|265,016 images, at least 3 questions per image, 10 ground truth answers per question, 3 plausible (but likely incorrect) answers per question.|[GitHub](https://github.com/GT-Vision-Lab/VQA)|

### Leaderboards

- [MMMU](https://mmmu-benchmark.github.io/#leaderboard)