## Other Benchmarks

| Title | Description | Link |
|---|---|---|
| [AGIEval](https://arxiv.org/pdf/2304.06364s) | Human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. | [GitHub](https://github.com/ruixiangcui/AGIEval) |
| [ARC-AGI-2](https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025) | Symbolic Interpretation, Compositional Reasoning, Contextual Rule Application | [Website](https://arcprize.org/blog/announcing-arc-agi-2-and-arc-prize-2025) |
| [ARC-Challenge](https://pgpbpadilla.github.io/chollet-arc-challenge) | AI2 Reasoning Challenge - A unique benchmark designed to measure AI skill acquisition and track progress towards achieving human-level AI. Test-takers are given a set of demonstration grid pairs. These serve as examples from which they must derive the output grid for the actual test. | [GitHub](https://github.com/fchollet/ARC/tree/master), [Lab42](https://lab42.global/arc/) |
| [BIG-Bench](https://arxiv.org/pdf/2206.04615) | 204 tasks, contributed by 450 authors across 132 institutions. | [Website](https://github.com/google/BIG-benchs)  |
| [Belebele](https://aclanthology.org/2024.acl-long.44.pdfs) | Evaluation of text models in high-, medium-, and low-resource languages. | [GitHub](https://github.com/facebookresearch/belebeles) |
| [BoolQ](https://arxiv.org/pdf/1905.10044) | Yes/noquestionsthat are naturally occurring. 15942 examples. | [GitHub](https://github.com/google-research-datasets/boolean-questionss) |
| [DarkBench](https://arxiv.org/pdf/2503.10728)| 660 prompts with manipulative techniques. | [GitHub](https://github.com/apartresearch/darkbench) |
| [DROP](https://aclanthology.org/N19-1246.pdf) | A 96k question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). | [Website](https://leaderboard.allenai.org/drop/submissions/get-starteds)|
| [FloRes](https://ai.meta.com/research/publications/the-flores-101-evaluation-benchmark-for-low-resource-and-multilingual-machine-translation/) | 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. | [Website](https://ai.meta.com/tools/flores/), [GitHub](https://github.com/facebookresearch/flores/) |
| [GAIA](https://arxiv.org/abs/2311.12983) | 466 real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. | [HuggingFace](https://huggingface.co/gaia-benchmark) |
| [Global-MMLU](https://arxiv.org/abs/2412.03304) | 42 languages, including English. This dataset combines machine translations for MMLU questions along with professional translations and crowd-sourced post-edit. | [Website](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/global_mmlu/README.md) |
| [GPQA](https://arxiv.org/abs/2311.12022) | 448 multiple-choice questions written by domain experts in biology, physics, and chemistry | [GitHub](https://github.com/idavidrein/gpqa) |
| [GQA](https://arxiv.org/abs/1902.09506)| 22M diverse visual reasoning questions, all come with functional programs that represent their semantics | [Website](https://cs.stanford.edu/people/dorarad/gqa/about.html) |
| [GSM-8K](https://arxiv.org/pdf/2110.14168) | 8.5K high quality linguistically diverse grade school math word problems - Arithmetic Reasoning | [GitHub](https://github.com/openai/grade-school-math) |
| [LiveBench](https://arxiv.org/abs/2406.19314) | Frequently-updated questions from recent information sources, scores answers automatically according to objective ground-truth values, and contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. | [GitHub](https://github.com/livebench/livebench), [Website](https://livebench.ai/) |
| [MATH](https://arxiv.org/pdf/2103.03874v2.pdf) | 12,500 challenging competition mathematics problems - Arithmetic Reasoning | [GitHub](https://github.com/hendrycks/math), [MATH-500](https://github.com/openai/prm800k/tree/main/prm800k/math_splits) |
| [MATH-500](https://github.com/openai/prm800k/tree/main/prm800k/math_splits) | [Subset of MATH](https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits) | [GitHub](https://github.com/openai/prm800k)|
| [MathArena](https://files.sri.inf.ethz.ch/matharena/usamo_report.pdf)|AIME, HMMT, BRUMO, USAMO|[GitHub](https://github.com/eth-sri/matharena)|
| [MathVisa](https://arxiv.org/pdf/2310.02255) | 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets | [GitHub](https://mathvista.github.io/) |
| [Mosaic Eval Gauntlet](https://github.com/mosaicml/llm-foundry/blob/main/scripts/eval/local_data/EVAL_GAUNTLET.md) | 35 different benchmarks collected from a variety of sources, and organized into 6 broad categories of competency that we expect good foundation models to have | |
| [MTEB](https://arxiv.org/abs/2210.07316) | Massive Text Embedding Benchmark | [GitHub](https://github.com/embeddings-benchmark/mteb), [HuggingFace Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) |
| [Multi-IF](https://arxiv.org/abs/2410.15553)| 4501 multi-turn and multilingual instructions | [GitHub](https://github.com/facebookresearch/Multi-IF) |
| [OSWorld](https://arxiv.org/abs/2404.07972)| 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications.| [Website](https://os-world.github.io/)|
| [PIQA](https://arxiv.org/abs/1911.11641) | Physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction | [HuggingFace](https://huggingface.co/datasets/ybisk/piqa) |
| [SocialIQA](https://arxiv.org/pdf/1904.09728) | 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations | [HuggingFace](https://huggingface.co/datasets/allenai/social_i_qa) |
| [TabMVP](https://promptpg.github.io/#paper) |38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data.| [Website](https://promptpg.github.io/)|
| [TriviaQA](https://arxiv.org/pdf/1705.03551) | 650K question-answer-evidence triples. | [Website](https://nlp.cs.washington.edu/triviaqa/) |
| [VisualQA](https://visualqa.org/)| Oppen-ended questions about 265016 images. ||
| [VisualWebArena](https://arxiv.org/abs/2401.13649)|A diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents.|[GitHub](https://github.com/web-arena-x/visualwebarena), [Website](https://jykoh.com/vwa)|
| [Webarena](https://arxiv.org/pdf/2307.13854.pdf)||[Website](https://webarena.dev/)|
| [WMT24++](https://arxiv.org/pdf/2502.12404v1) | We extend the WMT24 dataset to cover 55 languages by collecting new human-written references and postedits for 46 new languages and dialects in addition to post-edits of the references in 8 out of 9 languages in the original WMT24 dataset. | |
| [XL-Sum](https://arxiv.org/pdf/2106.13822) | Multilingual summarization dataset supporting 44 languages from BBC. | [GitHub](https://github.com/csebuetnlp/xl-sum) |
| [XQuAD](https://github.com/google-deepmind/xquad) | Evaluates cross-lingual question answering performance. The dataset consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 | [GitHub](https://github.com/google-deepmind/xquad)  |