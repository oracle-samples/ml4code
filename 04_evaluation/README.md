# Evaluation

## Metrics

|Title|Description|Link|
|---------|--------------------|---|
|Exact Match (EM)|Either the predicted text is an exact match to the expected result, which gets a score of 1, or it doesnâ€™t, which gets a score of 0.|-|
|Success Rate (SR)|Metric, which measures the proportation of task instances successfully completed|-|
|Execution Accuracy (EX)|EX is defined as the proportion of examples in the evaluation set for which the executed results of both the predicted and ground-truth SQLs are identical, relative to the overall number of SQLs|Yu et al. (2018), Li et al. (2024)|
|[pass@k]({{< ref "docs/04_evaluation/01_metrics/02_passk.md" >}})|the probability that at least one of the top k samples is correct, given that there are c-correct samples in total out of n-generated samples|[Data](https://github.com/openai/human-eval.git), OpenAI|
|[acc@k](https://arxiv.org/pdf/2302.00288)|measures the percentage of the target functions whose each oracle_context token is included by at least one of K samples generated by the LLM.|[Data](https://github.com/CoderEval/CoderEval), Huawei and Peking University|
|[CodeBLEU](https://arxiv.org/pdf/2009.10297.pdf)|consists of the original BLEU, the weighted n-gram match, the syntactic AST match, and the semantic data-flow match|[Data](https://github.com/microsoft/CodeXGLUE/tree/main/Code-Code/code-to-code-trans/evaluator/CodeBLEU), Microsoft and Peking University|
|[BLEU]({{< ref "docs/04_evaluation/01_metrics/01_bleu.md" >}})|compares a sentence against one or more reference sentences and tells how well does the candidate sentence matched the list of reference sentences|[ACL](https://aclanthology.org/P02-1040.pdf)|
|[ROUGE](https://aclanthology.org/W04-1013.pdf)|Recall-Oriented Understudy for Gisting Evaluation - ROUGE-N, ROUGE-S and ROUGE-L|[GitHub](https://github.com/pltrdy/rouge)|
